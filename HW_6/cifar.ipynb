{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользовался уже готовой арзитектурой и просто обучил ее. Была так же идея сделать более умную аугментацию, а не только флиппать иззображения. Нашел готовую библиотеку imgaug, в которой можно реалтзовывать pipeline, но он постоянно падал с исключениями и я не смог отдебажить. Но использовав AffineTransformation из OpenCV на другой архитектуре мне удалось увеличить accuracy на 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from lasagne import init\n",
    "from lasagne.layers import Conv2DLayer as ConvLayer\n",
    "from lasagne.layers import ElemwiseSumLayer\n",
    "from lasagne.layers import InputLayer\n",
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import GlobalPoolLayer\n",
    "from lasagne.layers import PadLayer\n",
    "from lasagne.layers import ExpressionLayer\n",
    "from lasagne.layers import NonlinearityLayer\n",
    "from lasagne.nonlinearities import softmax, rectify\n",
    "from lasagne.layers import batch_norm\n",
    "import lasagne.nonlinearities\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import string\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "def unpickle(file):\n",
    "    import cPickle\n",
    "    fo = open(file, 'rb')\n",
    "    dict = cPickle.load(fo)\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "from lasagne.layers import Conv2DLayer as ConvLayer\n",
    "from lasagne.layers import ElemwiseSumLayer\n",
    "from lasagne.layers import InputLayer\n",
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import GlobalPoolLayer\n",
    "from lasagne.layers import PadLayer\n",
    "from lasagne.layers import ExpressionLayer\n",
    "from lasagne.layers import NonlinearityLayer\n",
    "from lasagne.nonlinearities import softmax, rectify\n",
    "from lasagne.layers import batch_norm\n",
    "\n",
    "def build_cnn(input_var=None, n=5):\n",
    "    \n",
    "    # create a residual learning building block with two stacked 3x3 convlayers as in paper\n",
    "    def residual_block(l, increase_dim=False, projection=False):\n",
    "        input_num_filters = l.output_shape[1]\n",
    "        if increase_dim:\n",
    "            first_stride = (2,2)\n",
    "            out_num_filters = input_num_filters*2\n",
    "        else:\n",
    "            first_stride = (1,1)\n",
    "            out_num_filters = input_num_filters\n",
    "\n",
    "        stack_1 = batch_norm(ConvLayer(l, num_filters=out_num_filters, filter_size=(3,3), stride=first_stride, nonlinearity=rectify, pad='same', W=lasagne.init.HeNormal(gain='relu'), flip_filters=False))\n",
    "        stack_2 = batch_norm(ConvLayer(stack_1, num_filters=out_num_filters, filter_size=(3,3), stride=(1,1), nonlinearity=None, pad='same', W=lasagne.init.HeNormal(gain='relu'), flip_filters=False))\n",
    "        \n",
    "        # add shortcut connections\n",
    "        if increase_dim:\n",
    "            if projection:\n",
    "                # projection shortcut, as option B in paper\n",
    "                projection = batch_norm(ConvLayer(l, num_filters=out_num_filters, filter_size=(1,1), stride=(2,2), nonlinearity=None, pad='same', b=None, flip_filters=False))\n",
    "                block = NonlinearityLayer(ElemwiseSumLayer([stack_2, projection]),nonlinearity=rectify)\n",
    "            else:\n",
    "                # identity shortcut, as option A in paper\n",
    "                identity = ExpressionLayer(l, lambda X: X[:, :, ::2, ::2], lambda s: (s[0], s[1], s[2]//2, s[3]//2))\n",
    "                padding = PadLayer(identity, [out_num_filters//4,0,0], batch_ndim=1)\n",
    "                block = NonlinearityLayer(ElemwiseSumLayer([stack_2, padding]),nonlinearity=rectify)\n",
    "        else:\n",
    "            block = NonlinearityLayer(ElemwiseSumLayer([stack_2, l]),nonlinearity=rectify)\n",
    "        \n",
    "        return block\n",
    "\n",
    "    # Building the network\n",
    "    l_in = InputLayer(shape=(None, 3, 32, 32), input_var=input_var)\n",
    "\n",
    "    # first layer, output is 16 x 32 x 32\n",
    "    l = batch_norm(ConvLayer(l_in, num_filters=16, filter_size=(3,3), stride=(1,1), nonlinearity=rectify, pad='same', W=lasagne.init.HeNormal(gain='relu'), flip_filters=False))\n",
    "    \n",
    "    # first stack of residual blocks, output is 16 x 32 x 32\n",
    "    for _ in range(n):\n",
    "        l = residual_block(l)\n",
    "\n",
    "    # second stack of residual blocks, output is 32 x 16 x 16\n",
    "    l = residual_block(l, increase_dim=True)\n",
    "    for _ in range(1,n):\n",
    "        l = residual_block(l)\n",
    "\n",
    "    # third stack of residual blocks, output is 64 x 8 x 8\n",
    "    l = residual_block(l, increase_dim=True)\n",
    "    for _ in range(1,n):\n",
    "        l = residual_block(l)\n",
    "    \n",
    "    # average pooling\n",
    "    l = GlobalPoolLayer(l)\n",
    "\n",
    "    # fully connected layer\n",
    "    network = DenseLayer(\n",
    "            l, num_units=10,\n",
    "            W=lasagne.init.HeNormal(),\n",
    "            nonlinearity=softmax)\n",
    "\n",
    "    return network\n",
    "\n",
    "# ############################# Batch iterator ###############################\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False, augment=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        if augment:\n",
    "            # as in paper : \n",
    "            # pad feature arrays with 4 pixels on each side\n",
    "            # and do random cropping of 32x32\n",
    "            padded = np.pad(inputs[excerpt],((0,0),(0,0),(4,4),(4,4)),mode='constant')\n",
    "            random_cropped = np.zeros(inputs[excerpt].shape, dtype=np.float32)\n",
    "            crops = np.random.random_integers(0,high=8,size=(batchsize,2))\n",
    "            for r in range(batchsize):\n",
    "                random_cropped[r,:,:,:] = padded[r,:,crops[r,0]:(crops[r,0]+32),crops[r,1]:(crops[r,1]+32)]\n",
    "            inp_exc = random_cropped\n",
    "        else:\n",
    "            inp_exc = inputs[excerpt]\n",
    "\n",
    "        yield inp_exc, targets[excerpt]\n",
    "\n",
    "n=5\n",
    "num_epochs=82\n",
    "model=None\n",
    "\n",
    "# Load the dataset\n",
    "xs = []\n",
    "ys = []\n",
    "for j in range(5):\n",
    "    d = unpickle('cifar-10-batches-py/data_batch_'+`j+1`)\n",
    "    x = d['data']\n",
    "    y = d['labels']\n",
    "    xs.append(x)\n",
    "    ys.append(y)\n",
    "\n",
    "d = unpickle('cifar-10-batches-py/test_batch')\n",
    "xs.append(d['data'])\n",
    "ys.append(d['labels'])\n",
    "\n",
    "x = np.concatenate(xs)/np.float32(255)\n",
    "y = np.concatenate(ys)\n",
    "x = np.dstack((x[:, :1024], x[:, 1024:2048], x[:, 2048:]))\n",
    "x = x.reshape((x.shape[0], 32, 32, 3)).transpose(0,3,1,2)\n",
    "\n",
    "# subtract per-pixel mean\n",
    "pixel_mean = np.mean(x[0:50000],axis=0)\n",
    "x -= pixel_mean\n",
    "\n",
    "# create mirrored images\n",
    "X_train = x[0:50000,:,:,:]\n",
    "Y_train = y[0:50000]\n",
    "X_train_flip = X_train[:,:,:,::-1]\n",
    "Y_train_flip = Y_train\n",
    "X_train = np.concatenate((X_train,X_train_flip),axis=0)\n",
    "Y_train = np.concatenate((Y_train,Y_train_flip),axis=0)\n",
    "\n",
    "X_test = x[50000:,:,:,:]\n",
    "Y_test = y[50000:]\n",
    "\n",
    "data = dict(\n",
    "        X_train=lasagne.utils.floatX(X_train),\n",
    "        Y_train=Y_train.astype('int32'),\n",
    "        X_test = lasagne.utils.floatX(X_test),\n",
    "        Y_test = Y_test.astype('int32'),)\n",
    "\n",
    "X_train = data['X_train']\n",
    "Y_train = data['Y_train']\n",
    "X_test = data['X_test']\n",
    "Y_test = data['Y_test']\n",
    "\n",
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.ivector('targets')\n",
    "\n",
    "network = build_cnn(input_var, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "loss = loss.mean()\n",
    "all_layers = lasagne.layers.get_all_layers(network)\n",
    "l2_penalty = lasagne.regularization.regularize_layer_params(all_layers, lasagne.regularization.l2) * 0.0001\n",
    "loss = loss + l2_penalty\n",
    "\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "lr = 0.1\n",
    "sh_lr = theano.shared(lasagne.utils.floatX(lr))\n",
    "updates = lasagne.updates.momentum(\n",
    "        loss, params, learning_rate=sh_lr, momentum=0.9)\n",
    "\n",
    "\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                        target_var)\n",
    "test_loss = test_loss.mean()\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                  dtype=theano.config.floatX)\n",
    "\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Подготовили все и сделали структуру для вычислений, начнем теперь обучать сеть. В статье предлагается повторять по 5 раз блоки, и обучали они ее на 82 эпохах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_indices = np.arange(100000)\n",
    "    np.random.shuffle(train_indices)\n",
    "    X_train = X_train[train_indices,:,:,:]\n",
    "    Y_train = Y_train[train_indices]\n",
    "\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, Y_train, 128, shuffle=True, augment=True):\n",
    "        inputs, targets = batch\n",
    "        train_err += train_fn(inputs, targets)\n",
    "        train_batches += 1\n",
    "\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_test, Y_test, 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "\n",
    "    if (epoch+1) == 41 or (epoch+1) == 61:\n",
    "        new_lr = sh_lr.get_value() * 0.1\n",
    "        sh_lr.set_value(lasagne.utils.floatX(new_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.savez('cifar10_deep_residual_model.npz', *lasagne.layers.get_all_param_values(network))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сеть довольно быстро обуалась, уже на 4 эпохе на тесте была точность около 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate validation error of model:\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, Y_test, 500, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, acc = val_fn(inputs, targets)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посмотрим на качество на тесте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test loss:\t\t\t0.326712\n",
      "  test accuracy:\t\t92.65 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir cifar10\n",
    "# # !curl -o cifar-10-python.tar.gz https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "!tar -xvzf cifar-10-python.tar.gz -C cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 5110 on context None\n",
      "Mapped name None to device cuda: GeForce GTX 1080 (0000:08:00.0)\n"
     ]
    }
   ],
   "source": [
    "from lasagne import init\n",
    "from lasagne.layers import Conv2DLayer as ConvLayer\n",
    "#from lasagne.layers.dnn import Conv2DDNNLayer as ConvLayer\n",
    "from lasagne.layers import ElemwiseSumLayer\n",
    "from lasagne.layers import InputLayer\n",
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import GlobalPoolLayer\n",
    "from lasagne.layers import PadLayer\n",
    "from lasagne.layers import ExpressionLayer\n",
    "from lasagne.layers import NonlinearityLayer\n",
    "from lasagne.nonlinearities import softmax, rectify\n",
    "from lasagne.layers import batch_norm\n",
    "import lasagne.nonlinearities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cifar import load_CIFAR10\n",
    "# plt.rcParams['figure.figsize'] = (10.0, 8.0) \n",
    "\n",
    "# cifar10_dir = './cifar10/cifar-10-batches-py'\n",
    "# X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">First of all -- Checking Questions</h1> \n",
    "\n",
    "**Вопрос 1**: Чем отличаются современные сверточные сети от сетей 5 летней давности?\n",
    "\n",
    "Раньше сети имели более простую структуру, на данный момент сети имеют огроиное число слоев и другую структуру иногда, например, из-за проблем, что далеко от конца градиент при бэкпропе слабо доходит и делаются оутпуты не только 1 слой\n",
    "\n",
    "**Вопрос 2**: Какие неприятности могут возникнуть во время обучения современных нейросетей?\n",
    "\n",
    "Слишком много параметров для оптимизации, много слоев , где затухать градиент может.\n",
    "\n",
    "**Вопрос 3**: У вас есть очень маленький датасет из 100 картинок, классификация, но вы очень хотите использовать нейросеть, какие неприятности вас ждут и как их решить? что делать если первый вариант  решения не заработает?\n",
    "\n",
    "Это очень мало данных для обучения, если мы дадим даже искаженную на чуть-чуть или сдвинутую картинку, то скорее всего сеть не сможет нормально классифицировать, она будет искать именно такое изображение и переобцчится\n",
    "\n",
    "\n",
    "**Вопрос 4**: Как сделать стайл трансфер для музыки? oO\n",
    "\n",
    "Строится спектрограмма, там используется оконное преобразование Фурье и уже на спектрограмму накладывают стиль как и на картинку, только там по-другому нужно свертку делать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from theano import tensor as T\n",
    "from lasagne.nonlinearities import *\n",
    "\n",
    "# input_X = T.tensor4(\"X\")\n",
    "# target_y = T.vector(\"target Y integer\",dtype='int32')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Соберите нейронку: \n",
    "- Many times x (Conv+Pool)\n",
    "- Many small convolutions like 3x3\n",
    "- Batch Norm \n",
    "- Residual Connection\n",
    "- Data Augmentation \n",
    "- Learning rate Schedule \n",
    "- ...\n",
    "\n",
    "### Для вдохновения \n",
    "- http://torch.ch/blog/2015/07/30/cifar.html\n",
    "- https://github.com/szagoruyko/wide-residual-networks \n",
    "\n",
    "### Самое интересное\n",
    "- Для сдачи задания нужно набрать на точность тесте > **92.5**% (это займет много времени, торопитесь :) )\n",
    "- Для получения бонусных баллов > **95.0**%\n",
    "- Будет очень хорошо если вы придумаете свою архитектуру или сможете обучить что-то из вышеперечисленного :)\n",
    "- А для обучения всего этого добра вам будет куда удобнее использовать GPU на Amazon \n",
    "    - Инструкция https://github.com/persiyanov/ml-mipt/tree/master/amazon-howto \n",
    "    - Вам помогут tmux, CuDNN, ssh tunnel, nvidia-smi, ... \n",
    "    - Have fun :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lasagne.layers.dnn import Conv2DDNNLayer, MaxPool2DDNNLayer\n",
    "from nolearn.lasagne import NeuralNet\n",
    "from lasagne.layers import Conv2DLayer, MaxPool2DLayer, FeaturePoolLayer, DropoutLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пытался использовать другую архитектуру, но она не смогла дать не нужный резулттат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNet(\n",
    "    layers=[\n",
    "        (InputLayer, dict(name='in', shape=(None, 3, image_size, image_size))),\n",
    "\n",
    "        (Conv2DLayer, dict(name='l1c1', num_filters=32, filter_size=(3, 3), pad='same', W=init.Orthogonal(2**.5))),\n",
    "        (Conv2DLayer, dict(name='l1c2', num_filters=32, filter_size=(3, 3), pad='same', W=init.Orthogonal(2**.5))),\n",
    "        (Conv2DLayer, dict(name='l1c3', num_filters=32, filter_size=(3, 3), pad='same', W=init.Orthogonal(2**.5))),\n",
    "        (Conv2DLayer, dict(name='l1c4', num_filters=48, filter_size=(3, 3), pad='same', W=init.Orthogonal(2**.5))),\n",
    "        (Conv2DLayer, dict(name='l1c5', num_filters=48, filter_size=(3, 3), pad='same', W=init.Orthogonal(2**.5))),\n",
    "        (MaxPool2DLayer, dict(name='l1p', pool_size=(2,2), stride=(1,1))),\n",
    "\n",
    "        (Conv2DLayer, dict(name='l2c1', num_filters=80, filter_size=(3, 3), pad='same', W=init.Orthogonal(2**.5))),\n",
    "        (Conv2DLayer, dict(name='l2c2', num_filters=80, filter_size=(3, 3), pad='same', W=init.Orthogonal(2**.5))),\n",
    "        (Conv2DLayer, dict(name='l2c3', num_filters=80, filter_size=(3, 3), pad='same', W=init.Orthogonal(2**.5))),\n",
    "        (Conv2DLayer, dict(name='l2c4', num_filters=80, filter_size=(3, 3), pad='same', W=init.Orthogonal(2**.5))),\n",
    "        (Conv2DLayer, dict(name='l2c5', num_filters=80, filter_size=(3, 3), pad='same', W=init.Orthogonal(2**.5))),\n",
    "        (MaxPool2DLayer, dict(name='l2p', pool_size=(2,2), stride=(1,1))),\n",
    "        \n",
    "        (Conv2DLayer, dict(name='l3c1', num_filters=128, filter_size=(3, 3), pad='same', W=init.Orthogonal(2**.5))),\n",
    "        (Conv2DLayer, dict(name='l3c2', num_filters=128, filter_size=(3, 3), pad='same', W=init.Orthogonal(2**.5))),\n",
    "        (Conv2DLayer, dict(name='l3c3', num_filters=128, filter_size=(3, 3), pad='same', W=init.Orthogonal(2**.5))),\n",
    "        (Conv2DLayer, dict(name='l3c4', num_filters=128, filter_size=(3, 3), pad='same', W=init.Orthogonal(2**.5))),\n",
    "        (Conv2DLayer, dict(name='l3c5', num_filters=128, filter_size=(3, 3), pad='same', W=init.Orthogonal(2**.5))),\n",
    "        (MaxPool2DLayer, dict(name='l3p', pool_size=(8,8), stride=(1,1))),\n",
    "\n",
    "        (DenseLayer, dict(name='l4', num_units=1000)),\n",
    "\n",
    "        (DenseLayer, dict(name='out', num_units=n_classes, nonlinearity=lasagne.nonlinearities.softmax)),\n",
    "    ],\n",
    "    \n",
    "    regression=False,\n",
    "    objective_loss_function=lasagne.objectives.categorical_crossentropy,\n",
    "\n",
    "    update=lasagne.updates.adam,\n",
    "\n",
    "    batch_iterator_train=train_iterator,\n",
    "    batch_iterator_test=test_iterator,\n",
    "\n",
    "    on_epoch_finished=[\n",
    "        save_weights,\n",
    "        save_training_history,\n",
    "        plot_training_history\n",
    "    ],\n",
    "\n",
    "    verbose=10,\n",
    "    max_epochs=250,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вот и всё, пошли её учить"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заполните форму\n",
    "\n",
    "https://goo.gl/forms/EeadABISlVmdJqgr2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
